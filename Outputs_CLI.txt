(base) C:\Users\Administrator>D:

(base) D:\>cd Anum

(base) D:\Anum>cd MA-SAM

(base) D:\Anum\MA-SAM>conda create -n masam python=3.10.12
3 channel Terms of Service accepted
Channels:
 - defaults
Platform: win-64
Collecting package metadata (repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
    current version: 25.5.1
    latest version: 26.1.0

Please update conda by running

    $ conda update -n base -c defaults conda



## Package Plan ##

  environment location: C:\Users\ADMIN\.conda\envs\masam

  added / updated specs:
    - python=3.10.12


The following NEW packages will be INSTALLED:

  bzip2              pkgs/main/win-64::bzip2-1.0.8-h2bbff1b_6
  ca-certificates    pkgs/main/win-64::ca-certificates-2025.12.2-haa95532_0
  libffi             pkgs/main/win-64::libffi-3.4.4-hd77b12b_1
  libzlib            pkgs/main/win-64::libzlib-1.3.1-h02ab6af_0
  openssl            pkgs/main/win-64::openssl-3.0.19-hbb43b14_0
  packaging          pkgs/main/win-64::packaging-25.0-py310haa95532_1
  pip                pkgs/main/noarch::pip-25.3-pyhc872135_0
  python             pkgs/main/win-64::python-3.10.12-he1021f5_0
  setuptools         pkgs/main/win-64::setuptools-80.10.2-py310haa95532_0
  sqlite             pkgs/main/win-64::sqlite-3.51.1-hda9a48d_0
  tk                 pkgs/main/win-64::tk-8.6.15-hf199647_0
  tzdata             pkgs/main/noarch::tzdata-2025c-he532380_0
  ucrt               pkgs/main/win-64::ucrt-10.0.22621.0-haa95532_0
  vc                 pkgs/main/win-64::vc-14.3-h2df5915_10
  vc14_runtime       pkgs/main/win-64::vc14_runtime-14.44.35208-h4927774_10
  vs2015_runtime     pkgs/main/win-64::vs2015_runtime-14.44.35208-ha6b5a95_10
  wheel              pkgs/main/win-64::wheel-0.46.3-py310haa95532_0
  xz                 pkgs/main/win-64::xz-5.6.4-h4754444_1
  zlib               pkgs/main/win-64::zlib-1.3.1-h02ab6af_0

Proceed ([y]/n)? y

Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
#
# To activate this environment, use
#
#     $ conda activate masam
#
# To deactivate an active environment, use
#
#     $ conda deactivate


(base) D:\Anum\MA-SAM>conda activate masam

(masam) D:\Anum\MA-SAM>conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
3 channel Terms of Service accepted
Channels:
 - pytorch
 - nvidia
 - defaults
Platform: win-64
Collecting package metadata (repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
    current version: 25.5.1
    latest version: 26.1.0

Please update conda by running

    $ conda update -n base -c defaults conda



## Package Plan ##

  environment location: C:\Users\ADMIN\.conda\envs\masam

  added / updated specs:
    - pytorch
    - pytorch-cuda=11.7
    - torchaudio
    - torchvision


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    brotlicffi-1.2.0.0         |  py310h885b0b7_0         346 KB
    certifi-2026.01.04         |  py310haa95532_0         148 KB
    cffi-2.0.0                 |  py310h02ab6af_1         241 KB
    charset-normalizer-3.4.4   |  py310haa95532_0         106 KB
    cuda-cccl-13.1.115         |       h415d894_0          18 KB  nvidia
    cuda-cccl_win-64-13.1.115  |       hc667259_0         1.2 MB  nvidia
    cuda-cudart-11.7.99        |                0         1.4 MB  nvidia
    cuda-cudart-dev-11.7.99    |                0         711 KB  nvidia
    cuda-cupti-11.7.101        |                0        10.2 MB  nvidia
    cuda-libraries-11.7.1      |                0           1 KB  nvidia
    cuda-libraries-dev-11.7.1  |                0           1 KB  nvidia
    cuda-nvrtc-11.7.99         |                0        71.9 MB  nvidia
    cuda-nvrtc-dev-11.7.99     |                0        14.3 MB  nvidia
    cuda-nvtx-11.7.91          |                0          43 KB  nvidia
    cuda-runtime-11.7.1        |                0           1 KB  nvidia
    cuda-version-13.1          |       hd92462c_3          17 KB  nvidia
    filelock-3.20.3            |  py310haa95532_0          34 KB
    gmpy2-2.2.2                |  py310h8598115_0         211 KB
    idna-3.11                  |  py310haa95532_0         196 KB
    jinja2-3.1.6               |  py310haa95532_0         279 KB
    lcms2-2.16                 |       hb4a4139_0         566 KB
    lerc-3.0                   |       hd77b12b_0         120 KB
    libcublas-11.10.3.66       |                0          24 KB  nvidia
    libcublas-dev-11.10.3.66   |                0       282.4 MB  nvidia
    libcufft-10.7.2.124        |                0           6 KB  nvidia
    libcufft-dev-10.7.2.124    |                0       250.1 MB  nvidia
    libcurand-10.4.1.81        |       hd7d98ca_0        43.9 MB  nvidia
    libcurand-dev-10.4.1.81    |       hd7d98ca_0         264 KB  nvidia
    libcusolver-11.4.0.1       |                0          29 KB  nvidia
    libcusolver-dev-11.4.0.1   |                0        76.5 MB  nvidia
    libcusparse-11.7.4.91      |                0          13 KB  nvidia
    libcusparse-dev-11.7.4.91  |                0       149.6 MB  nvidia
    libdeflate-1.17            |       h2bbff1b_1         153 KB
    libnpp-11.7.4.75           |                0         294 KB  nvidia
    libnpp-dev-11.7.4.75       |                0       125.6 MB  nvidia
    libnvjpeg-11.8.0.2         |                0           4 KB  nvidia
    libnvjpeg-dev-11.8.0.2     |                0         1.7 MB  nvidia
    libtiff-4.5.1              |       hd77b12b_0         1.1 MB
    libwebp-1.3.2              |       hbc33d0d_0          73 KB
    libwebp-base-1.3.2         |       h3d04722_1         303 KB
    markupsafe-3.0.2           |  py310h827c3e9_0          36 KB
    mkl-service-2.4.0          |  py310h827c3e9_2          66 KB
    mkl_fft-1.3.11             |  py310h827c3e9_0         168 KB
    mkl_random-1.2.8           |  py310hc64d2fc_0         257 KB
    mpmath-1.3.0               |  py310haa95532_0         834 KB
    networkx-3.4.2             |  py310haa95532_0         2.5 MB
    numpy-2.0.1                |  py310h055cbcc_1          11 KB
    numpy-base-2.0.1           |  py310h65a83cf_1         9.1 MB
    openjpeg-2.5.2             |       hae555c5_0         268 KB
    pillow-11.1.0              |  py310h096bfcc_0         770 KB
    pycparser-2.23             |  py310haa95532_0         213 KB
    pysocks-1.7.1              |  py310haa95532_1          29 KB
    pytorch-2.5.1              |     py3.10_cpu_0       147.4 MB  pytorch
    pytorch-cuda-11.7          |       h16d0643_5           4 KB  pytorch
    pyyaml-6.0.3               |  py310hb9a58be_0         203 KB
    requests-2.32.5            |  py310haa95532_1         142 KB
    sympy-1.14.0               |  py310haa95532_1        10.6 MB
    torchaudio-2.5.1           |        py310_cpu         5.8 MB  pytorch
    torchvision-0.20.1         |        py310_cpu         6.5 MB  pytorch
    typing_extensions-4.15.0   |  py310haa95532_0          79 KB
    urllib3-2.6.3              |  py310haa95532_0         297 KB
    win_inet_pton-1.1.0        |  py310haa95532_1           9 KB
    ------------------------------------------------------------
                                           Total:        1.19 GB

The following NEW packages will be INSTALLED:

  blas               pkgs/main/win-64::blas-1.0-mkl
  brotlicffi         pkgs/main/win-64::brotlicffi-1.2.0.0-py310h885b0b7_0
  certifi            pkgs/main/win-64::certifi-2026.01.04-py310haa95532_0
  cffi               pkgs/main/win-64::cffi-2.0.0-py310h02ab6af_1
  charset-normalizer pkgs/main/win-64::charset-normalizer-3.4.4-py310haa95532_0
  cuda-cccl          nvidia/win-64::cuda-cccl-13.1.115-h415d894_0
  cuda-cccl_win-64   nvidia/noarch::cuda-cccl_win-64-13.1.115-hc667259_0
  cuda-cudart        nvidia/win-64::cuda-cudart-11.7.99-0
  cuda-cudart-dev    nvidia/win-64::cuda-cudart-dev-11.7.99-0
  cuda-cupti         nvidia/win-64::cuda-cupti-11.7.101-0
  cuda-libraries     nvidia/win-64::cuda-libraries-11.7.1-0
  cuda-libraries-dev nvidia/win-64::cuda-libraries-dev-11.7.1-0
  cuda-nvrtc         nvidia/win-64::cuda-nvrtc-11.7.99-0
  cuda-nvrtc-dev     nvidia/win-64::cuda-nvrtc-dev-11.7.99-0
  cuda-nvtx          nvidia/win-64::cuda-nvtx-11.7.91-0
  cuda-runtime       nvidia/win-64::cuda-runtime-11.7.1-0
  cuda-version       nvidia/noarch::cuda-version-13.1-hd92462c_3
  filelock           pkgs/main/win-64::filelock-3.20.3-py310haa95532_0
  freetype           pkgs/main/win-64::freetype-2.14.1-hfbffc0b_0
  giflib             pkgs/main/win-64::giflib-5.2.2-h7edc060_0
  gmp                pkgs/main/win-64::gmp-6.3.0-h537511b_0
  gmpy2              pkgs/main/win-64::gmpy2-2.2.2-py310h8598115_0
  idna               pkgs/main/win-64::idna-3.11-py310haa95532_0
  intel-openmp       pkgs/main/win-64::intel-openmp-2023.1.0-h59b6b97_46320
  jinja2             pkgs/main/win-64::jinja2-3.1.6-py310haa95532_0
  jpeg               pkgs/main/win-64::jpeg-9f-ha349fce_0
  lcms2              pkgs/main/win-64::lcms2-2.16-hb4a4139_0
  lerc               pkgs/main/win-64::lerc-3.0-hd77b12b_0
  libcublas          nvidia/win-64::libcublas-11.10.3.66-0
  libcublas-dev      nvidia/win-64::libcublas-dev-11.10.3.66-0
  libcufft           nvidia/win-64::libcufft-10.7.2.124-0
  libcufft-dev       nvidia/win-64::libcufft-dev-10.7.2.124-0
  libcurand          nvidia/win-64::libcurand-10.4.1.81-hd7d98ca_0
  libcurand-dev      nvidia/win-64::libcurand-dev-10.4.1.81-hd7d98ca_0
  libcusolver        nvidia/win-64::libcusolver-11.4.0.1-0
  libcusolver-dev    nvidia/win-64::libcusolver-dev-11.4.0.1-0
  libcusparse        nvidia/win-64::libcusparse-11.7.4.91-0
  libcusparse-dev    nvidia/win-64::libcusparse-dev-11.7.4.91-0
  libdeflate         pkgs/main/win-64::libdeflate-1.17-h2bbff1b_1
  libjpeg-turbo      pkgs/main/win-64::libjpeg-turbo-2.0.0-h196d8e1_0
  libnpp             nvidia/win-64::libnpp-11.7.4.75-0
  libnpp-dev         nvidia/win-64::libnpp-dev-11.7.4.75-0
  libnvjpeg          nvidia/win-64::libnvjpeg-11.8.0.2-0
  libnvjpeg-dev      nvidia/win-64::libnvjpeg-dev-11.8.0.2-0
  libpng             pkgs/main/win-64::libpng-1.6.54-ha15c746_0
  libtiff            pkgs/main/win-64::libtiff-4.5.1-hd77b12b_0
  libuv              pkgs/main/win-64::libuv-1.48.0-h827c3e9_0
  libwebp            pkgs/main/win-64::libwebp-1.3.2-hbc33d0d_0
  libwebp-base       pkgs/main/win-64::libwebp-base-1.3.2-h3d04722_1
  lz4-c              pkgs/main/win-64::lz4-c-1.9.4-h2bbff1b_1
  markupsafe         pkgs/main/win-64::markupsafe-3.0.2-py310h827c3e9_0
  mkl                pkgs/main/win-64::mkl-2023.1.0-h6b88ed4_46358
  mkl-service        pkgs/main/win-64::mkl-service-2.4.0-py310h827c3e9_2
  mkl_fft            pkgs/main/win-64::mkl_fft-1.3.11-py310h827c3e9_0
  mkl_random         pkgs/main/win-64::mkl_random-1.2.8-py310hc64d2fc_0
  mpc                pkgs/main/win-64::mpc-1.3.1-h827c3e9_0
  mpfr               pkgs/main/win-64::mpfr-4.2.1-h56c3642_0
  mpmath             pkgs/main/win-64::mpmath-1.3.0-py310haa95532_0
  networkx           pkgs/main/win-64::networkx-3.4.2-py310haa95532_0
  numpy              pkgs/main/win-64::numpy-2.0.1-py310h055cbcc_1
  numpy-base         pkgs/main/win-64::numpy-base-2.0.1-py310h65a83cf_1
  openjpeg           pkgs/main/win-64::openjpeg-2.5.2-hae555c5_0
  pillow             pkgs/main/win-64::pillow-11.1.0-py310h096bfcc_0
  pycparser          pkgs/main/win-64::pycparser-2.23-py310haa95532_0
  pysocks            pkgs/main/win-64::pysocks-1.7.1-py310haa95532_1
  pytorch            pytorch/win-64::pytorch-2.5.1-py3.10_cpu_0
  pytorch-cuda       pytorch/win-64::pytorch-cuda-11.7-h16d0643_5
  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cpu
  pyyaml             pkgs/main/win-64::pyyaml-6.0.3-py310hb9a58be_0
  requests           pkgs/main/win-64::requests-2.32.5-py310haa95532_1
  sympy              pkgs/main/win-64::sympy-1.14.0-py310haa95532_1
  tbb                pkgs/main/win-64::tbb-2021.8.0-h59b6b97_0
  torchaudio         pytorch/win-64::torchaudio-2.5.1-py310_cpu
  torchvision        pytorch/win-64::torchvision-0.20.1-py310_cpu
  typing_extensions  pkgs/main/win-64::typing_extensions-4.15.0-py310haa95532_0
  urllib3            pkgs/main/win-64::urllib3-2.6.3-py310haa95532_0
  win_inet_pton      pkgs/main/win-64::win_inet_pton-1.1.0-py310haa95532_1
  yaml               pkgs/main/win-64::yaml-0.2.5-he774522_0
  zstd               pkgs/main/win-64::zstd-1.5.7-h56299aa_0


Proceed ([y]/n)? y


Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done

(masam) D:\Anum\MA-SAM>pip install -r requirements.txt
Collecting einops==0.6.1 (from -r requirements.txt (line 1))
  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)
Collecting h5py==3.8.0 (from -r requirements.txt (line 2))
  Downloading h5py-3.8.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)
Collecting icecream==2.1.3 (from -r requirements.txt (line 3))
  Downloading icecream-2.1.3-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting imageio==2.28.1 (from -r requirements.txt (line 4))
  Downloading imageio-2.28.1-py3-none-any.whl.metadata (4.7 kB)
Collecting MedPy==0.4.0 (from -r requirements.txt (line 5))
  Downloading MedPy-0.4.0.tar.gz (151 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting nibabel==5.1.0 (from -r requirements.txt (line 6))
  Downloading nibabel-5.1.0-py3-none-any.whl.metadata (8.7 kB)
Collecting monai==1.1.0 (from -r requirements.txt (line 7))
  Downloading monai-1.1.0-202212191849-py3-none-any.whl.metadata (9.6 kB)
Collecting numpy==1.23.1 (from -r requirements.txt (line 8))
  Downloading numpy-1.23.1-cp310-cp310-win_amd64.whl.metadata (2.2 kB)
Collecting opencv_python==4.7.0.72 (from -r requirements.txt (line 9))
  Downloading opencv_python-4.7.0.72-cp37-abi3-win_amd64.whl.metadata (18 kB)
Collecting pycocotools==2.0.6 (from -r requirements.txt (line 10))
  Downloading pycocotools-2.0.6.tar.gz (24 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting safetensors==0.3.1 (from -r requirements.txt (line 11))
  Downloading safetensors-0.3.1-cp310-cp310-win_amd64.whl.metadata (4.6 kB)
Collecting scipy==1.10.1 (from -r requirements.txt (line 12))
  Downloading scipy-1.10.1-cp310-cp310-win_amd64.whl.metadata (58 kB)
Collecting SimpleITK==2.2.1 (from -r requirements.txt (line 13))
  Downloading SimpleITK-2.2.1-cp310-cp310-win_amd64.whl.metadata (8.2 kB)
Collecting tensorboardX==2.6 (from -r requirements.txt (line 14))
  Downloading tensorboardX-2.6-py2.py3-none-any.whl.metadata (5.4 kB)
Collecting tqdm==4.65.0 (from -r requirements.txt (line 15))
  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)
Collecting ml-collections==0.1.1 (from -r requirements.txt (line 16))
  Downloading ml_collections-0.1.1.tar.gz (77 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting onnx==1.14.0 (from -r requirements.txt (line 18))
  Downloading onnx-1.14.0-cp310-cp310-win_amd64.whl.metadata (15 kB)
Collecting onnxruntime==1.14.1 (from -r requirements.txt (line 19))
  Downloading onnxruntime-1.14.1-cp310-cp310-win_amd64.whl.metadata (4.0 kB)
Collecting pandas==2.0.2 (from -r requirements.txt (line 20))
  Downloading pandas-2.0.2-cp310-cp310-win_amd64.whl.metadata (18 kB)
Collecting colorama>=0.3.9 (from icecream==2.1.3->-r requirements.txt (line 3))
  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)
Collecting pygments>=2.2.0 (from icecream==2.1.3->-r requirements.txt (line 3))
  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)
Collecting executing>=0.3.1 (from icecream==2.1.3->-r requirements.txt (line 3))
  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)
Collecting asttokens>=2.0.1 (from icecream==2.1.3->-r requirements.txt (line 3))
  Downloading asttokens-3.0.1-py3-none-any.whl.metadata (4.9 kB)
Requirement already satisfied: pillow>=8.3.2 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from imageio==2.28.1->-r requirements.txt (line 4)) (11.1.0)
Requirement already satisfied: packaging>=17 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from nibabel==5.1.0->-r requirements.txt (line 6)) (25.0)
Requirement already satisfied: torch>=1.8 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from monai==1.1.0->-r requirements.txt (line 7)) (2.5.1)
Collecting matplotlib>=2.1.0 (from pycocotools==2.0.6->-r requirements.txt (line 10))
  Using cached matplotlib-3.10.8-cp310-cp310-win_amd64.whl.metadata (52 kB)
Collecting protobuf<4,>=3.8.0 (from tensorboardX==2.6->-r requirements.txt (line 14))
  Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl.metadata (698 bytes)
Collecting absl-py (from ml-collections==0.1.1->-r requirements.txt (line 16))
  Downloading absl_py-2.4.0-py3-none-any.whl.metadata (3.3 kB)
Requirement already satisfied: PyYAML in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from ml-collections==0.1.1->-r requirements.txt (line 16)) (6.0.3)
Collecting six (from ml-collections==0.1.1->-r requirements.txt (line 16))
  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting contextlib2 (from ml-collections==0.1.1->-r requirements.txt (line 16))
  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)
Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from onnx==1.14.0->-r requirements.txt (line 18)) (4.15.0)
Collecting coloredlogs (from onnxruntime==1.14.1->-r requirements.txt (line 19))
  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)
Collecting flatbuffers (from onnxruntime==1.14.1->-r requirements.txt (line 19))
  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)
Requirement already satisfied: sympy in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from onnxruntime==1.14.1->-r requirements.txt (line 19)) (1.14.0)
Collecting python-dateutil>=2.8.2 (from pandas==2.0.2->-r requirements.txt (line 20))
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pytz>=2020.1 (from pandas==2.0.2->-r requirements.txt (line 20))
  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.1 (from pandas==2.0.2->-r requirements.txt (line 20))
  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting contourpy>=1.0.1 (from matplotlib>=2.1.0->pycocotools==2.0.6->-r requirements.txt (line 10))
  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)
Collecting cycler>=0.10 (from matplotlib>=2.1.0->pycocotools==2.0.6->-r requirements.txt (line 10))
  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)
Collecting fonttools>=4.22.0 (from matplotlib>=2.1.0->pycocotools==2.0.6->-r requirements.txt (line 10))
  Using cached fonttools-4.61.1-cp310-cp310-win_amd64.whl.metadata (116 kB)
Collecting kiwisolver>=1.3.1 (from matplotlib>=2.1.0->pycocotools==2.0.6->-r requirements.txt (line 10))
  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)
Collecting pyparsing>=3 (from matplotlib>=2.1.0->pycocotools==2.0.6->-r requirements.txt (line 10))
  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)
Requirement already satisfied: filelock in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torch>=1.8->monai==1.1.0->-r requirements.txt (line 7)) (3.20.3)
Requirement already satisfied: networkx in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torch>=1.8->monai==1.1.0->-r requirements.txt (line 7)) (3.4.2)
Requirement already satisfied: jinja2 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torch>=1.8->monai==1.1.0->-r requirements.txt (line 7)) (3.1.6)
Collecting fsspec (from torch>=1.8->monai==1.1.0->-r requirements.txt (line 7))
  Using cached fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)
Collecting sympy (from onnxruntime==1.14.1->-r requirements.txt (line 19))
  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from sympy->onnxruntime==1.14.1->-r requirements.txt (line 19)) (1.3.0)
Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.14.1->-r requirements.txt (line 19))
  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)
Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime==1.14.1->-r requirements.txt (line 19))
  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)
Requirement already satisfied: MarkupSafe>=2.0 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from jinja2->torch>=1.8->monai==1.1.0->-r requirements.txt (line 7)) (3.0.2)
Downloading einops-0.6.1-py3-none-any.whl (42 kB)
Downloading h5py-3.8.0-cp310-cp310-win_amd64.whl (2.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 5.8 MB/s  0:00:00
Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)
Downloading imageio-2.28.1-py3-none-any.whl (3.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 2.5 MB/s  0:00:01
Downloading nibabel-5.1.0-py3-none-any.whl (3.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 3.2 MB/s  0:00:00
Downloading monai-1.1.0-202212191849-py3-none-any.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 4.5 MB/s  0:00:00
Downloading numpy-1.23.1-cp310-cp310-win_amd64.whl (14.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/14.6 MB 3.9 MB/s  0:00:03
Downloading opencv_python-4.7.0.72-cp37-abi3-win_amd64.whl (38.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.2/38.2 MB 10.9 MB/s  0:00:03
Downloading safetensors-0.3.1-cp310-cp310-win_amd64.whl (263 kB)
Downloading scipy-1.10.1-cp310-cp310-win_amd64.whl (42.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.5/42.5 MB 7.3 MB/s  0:00:05
Downloading SimpleITK-2.2.1-cp310-cp310-win_amd64.whl (27.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 27.0/27.0 MB 3.6 MB/s  0:00:07
Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)
Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)
Downloading onnx-1.14.0-cp310-cp310-win_amd64.whl (13.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/13.3 MB 4.0 MB/s  0:00:03
Downloading onnxruntime-1.14.1-cp310-cp310-win_amd64.whl (6.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 4.2 MB/s  0:00:01
Downloading pandas-2.0.2-cp310-cp310-win_amd64.whl (10.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/10.7 MB 4.5 MB/s  0:00:02
Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 904.0/904.0 kB 40.1 MB/s  0:00:00
Downloading asttokens-3.0.1-py3-none-any.whl (27 kB)
Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)
Using cached matplotlib-3.10.8-cp310-cp310-win_amd64.whl (8.1 MB)
Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)
Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)
Using cached fonttools-4.61.1-cp310-cp310-win_amd64.whl (1.6 MB)
Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)
Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)
Downloading pyparsing-3.3.2-py3-none-any.whl (122 kB)
Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)
Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)
Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 76.0 MB/s  0:00:00
Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)
Downloading absl_py-2.4.0-py3-none-any.whl (135 kB)
Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)
Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)
Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)
Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)
Using cached fsspec-2026.2.0-py3-none-any.whl (202 kB)
Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)
Building wheels for collected packages: MedPy, pycocotools, ml-collections
  Building wheel for MedPy (pyproject.toml) ... done
  Created wheel for MedPy: filename=medpy-0.4.0-py3-none-any.whl size=216002 sha256=cffce1a7e12822a8c716a1eee3efe79b8f871f891e47307fcb517c85b874d1e3
  Stored in directory: c:\users\ADMIN\appdata\local\pip\cache\wheels\d4\32\c7\6380ab2edb8cca018d39a0f1d43250fd9791922c963117de46
  Building wheel for pycocotools (pyproject.toml) ... done
  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp310-cp310-win_amd64.whl size=77686 sha256=ae35297bb06a2f5b5b3e83ed705fe2b3bba968559777ffe327fe55776ddef65b
  Stored in directory: c:\users\ADMIN\appdata\local\pip\cache\wheels\58\e6\f9\f87c8f8be098b51b616871315318329cae12cdb618f4caac93
  Building wheel for ml-collections (pyproject.toml) ... done
  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94706 sha256=23cc044bf33ac12a375a750e0b45439b4b20468f57d3fdbdfe09c261e74f63b7
  Stored in directory: c:\users\ADMIN\appdata\local\pip\cache\wheels\7b\89\c9\a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe
Successfully built MedPy pycocotools ml-collections
Installing collected packages: SimpleITK, safetensors, pytz, flatbuffers, tzdata, sympy, six, pyreadline3, pyparsing, pygments, protobuf, numpy, kiwisolver, fsspec, fonttools, executing, einops, cycler, contextlib2, colorama, asttokens, absl-py, tqdm, tensorboardX, scipy, python-dateutil, opencv_python, onnx, nibabel, ml-collections, imageio, icecream, humanfriendly, h5py, contourpy, pandas, monai, MedPy, matplotlib, coloredlogs, pycocotools, onnxruntime
  Attempting uninstall: sympy
    Found existing installation: sympy 1.14.0
    Uninstalling sympy-1.14.0:
      Successfully uninstalled sympy-1.14.0
  Attempting uninstall: numpy
    Found existing installation: numpy 2.0.1
    Uninstalling numpy-2.0.1:
      Successfully uninstalled numpy-2.0.1
Successfully installed MedPy-0.4.0 SimpleITK-2.2.1 absl-py-2.4.0 asttokens-3.0.1 colorama-0.4.6 coloredlogs-15.0.1 contextlib2-21.6.0 contourpy-1.3.2 cycler-0.12.1 einops-0.6.1 executing-2.2.1 flatbuffers-25.12.19 fonttools-4.61.1 fsspec-2026.2.0 h5py-3.8.0 humanfriendly-10.0 icecream-2.1.3 imageio-2.28.1 kiwisolver-1.4.9 matplotlib-3.10.8 ml-collections-0.1.1 monai-1.1.0 nibabel-5.1.0 numpy-1.23.1 onnx-1.14.0 onnxruntime-1.14.1 opencv_python-4.7.0.72 pandas-2.0.2 protobuf-3.20.3 pycocotools-2.0.6 pygments-2.19.2 pyparsing-3.3.2 pyreadline3-3.5.4 python-dateutil-2.9.0.post0 pytz-2025.2 safetensors-0.3.1 scipy-1.10.1 six-1.17.0 sympy-1.13.1 tensorboardX-2.6 tqdm-4.65.0 tzdata-2025.3

(masam) D:\Anum\MA-SAM>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
Looking in indexes: https://download.pytorch.org/whl/cu121
Collecting torch
  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp310-cp310-win_amd64.whl (2449.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 GB 7.7 MB/s  0:04:20
Collecting torchvision
  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-win_amd64.whl (6.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/6.1 MB 18.5 MB/s  0:00:00
Collecting torchaudio
  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-win_amd64.whl (4.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/4.1 MB 31.0 MB/s  0:00:00
Requirement already satisfied: filelock in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torch) (3.20.3)
Requirement already satisfied: typing-extensions>=4.8.0 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torch) (4.15.0)
Requirement already satisfied: networkx in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torch) (3.4.2)
Requirement already satisfied: jinja2 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torch) (2026.2.0)
Requirement already satisfied: sympy==1.13.1 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: numpy in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torchvision) (1.23.1)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from torchvision) (11.1.0)
Requirement already satisfied: MarkupSafe>=2.0 in c:\users\ADMIN\.conda\envs\masam\lib\site-packages (from jinja2->torch) (3.0.2)
Installing collected packages: torch, torchvision, torchaudio
Successfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121

(masam) D:\Anum\MA-SAM\Processed_data_nii>tree /f
Folder PATH listing for volume New Volume
Volume serial number is E8FC-0CB1
D:.
├───BIDMC
│       Case00.nii.gz
│       Case00_segmentation.nii.gz
│       Case02.nii.gz
│       Case02_segmentation.nii.gz
│       Case03.nii.gz
│       Case03_segmentation.nii.gz
│       Case04.nii.gz
│       Case04_segmentation.nii.gz
│       Case05.nii.gz
│       Case05_segmentation.nii.gz
│       Case06.nii.gz
│       Case06_segmentation.nii.gz
│       Case07.nii.gz
│       Case07_segmentation.nii.gz
│       Case08.nii.gz
│       Case08_segmentation.nii.gz
│       Case09.nii.gz
│       Case09_segmentation.nii.gz
│       Case10.nii.gz
│       Case10_segmentation.nii.gz
│       Case11.nii.gz
│       Case11_segmentation.nii.gz
│       Case12.nii.gz
│       Case12_segmentation.nii.gz
│
├───BMC
│       Case00.nii.gz
│       Case00_Segmentation.nii.gz
│       Case01.nii.gz
│       Case01_Segmentation.nii.gz
│       Case02.nii.gz
│       Case02_Segmentation.nii.gz
│       Case03.nii.gz
│       Case03_Segmentation.nii.gz
│       Case04.nii.gz
│       Case04_Segmentation.nii.gz
│       Case05.nii.gz
│       Case05_Segmentation.nii.gz
│       Case06.nii.gz
│       Case06_Segmentation.nii.gz
│       Case07.nii.gz
│       Case07_Segmentation.nii.gz
│       Case08.nii.gz
│       Case08_Segmentation.nii.gz
│       Case09.nii.gz
│       Case09_Segmentation.nii.gz
│       Case10.nii.gz
│       Case10_Segmentation.nii.gz
│       Case11.nii.gz
│       Case11_Segmentation.nii.gz
│       Case12.nii.gz
│       Case12_Segmentation.nii.gz
│       Case13.nii.gz
│       Case13_Segmentation.nii.gz
│       Case14.nii.gz
│       Case14_Segmentation.nii.gz
│       Case15.nii.gz
│       Case15_Segmentation.nii.gz
│       Case16.nii.gz
│       Case16_Segmentation.nii.gz
│       Case17.nii.gz
│       Case17_Segmentation.nii.gz
│       Case18.nii.gz
│       Case18_Segmentation.nii.gz
│       Case19.nii.gz
│       Case19_Segmentation.nii.gz
│       Case20.nii.gz
│       Case20_Segmentation.nii.gz
│       Case21.nii.gz
│       Case21_Segmentation.nii.gz
│       Case22.nii.gz
│       Case22_Segmentation.nii.gz
│       Case23.nii.gz
│       Case23_Segmentation.nii.gz
│       Case24.nii.gz
│       Case24_Segmentation.nii.gz
│       Case25.nii.gz
│       Case25_Segmentation.nii.gz
│       Case26.nii.gz
│       Case26_Segmentation.nii.gz
│       Case27.nii.gz
│       Case27_Segmentation.nii.gz
│       Case28.nii.gz
│       Case28_Segmentation.nii.gz
│       Case29.nii.gz
│       Case29_Segmentation.nii.gz
│
├───HK
│       Case38.nii.gz
│       Case38_segmentation.nii.gz
│       Case39.nii.gz
│       Case39_segmentation.nii.gz
│       Case40.nii.gz
│       Case40_segmentation.nii.gz
│       Case41.nii.gz
│       Case41_segmentation.nii.gz
│       Case42.nii.gz
│       Case42_segmentation.nii.gz
│       Case43.nii.gz
│       Case43_segmentation.nii.gz
│       Case44.nii.gz
│       Case44_segmentation.nii.gz
│       Case45.nii.gz
│       Case45_segmentation.nii.gz
│       Case46.nii.gz
│       Case46_segmentation.nii.gz
│       Case47.nii.gz
│       Case47_segmentation.nii.gz
│       Case48.nii.gz
│       Case48_segmentation.nii.gz
│       Case49.nii.gz
│       Case49_segmentation.nii.gz
│
├───I2CVB
│       Case00.nii.gz
│       Case00_segmentation.nii.gz
│       Case01.nii.gz
│       Case01_segmentation.nii.gz
│       Case02.nii.gz
│       Case02_segmentation.nii.gz
│       Case03.nii.gz
│       Case03_segmentation.nii.gz
│       Case04.nii.gz
│       Case04_segmentation.nii.gz
│       Case05.nii.gz
│       Case05_segmentation.nii.gz
│       Case06.nii.gz
│       Case06_segmentation.nii.gz
│       Case07.nii.gz
│       Case07_segmentation.nii.gz
│       Case08.nii.gz
│       Case08_segmentation.nii.gz
│       Case09.nii.gz
│       Case09_segmentation.nii.gz
│       Case10.nii.gz
│       Case10_segmentation.nii.gz
│       Case11.nii.gz
│       Case11_segmentation.nii.gz
│       Case12.nii.gz
│       Case12_segmentation.nii.gz
│       Case13.nii.gz
│       Case13_segmentation.nii.gz
│       Case14.nii.gz
│       Case14_segmentation.nii.gz
│       Case15.nii.gz
│       Case15_segmentation.nii.gz
│       Case16.nii.gz
│       Case16_segmentation.nii.gz
│       Case17.nii.gz
│       Case17_segmentation.nii.gz
│       Case18.nii.gz
│       Case18_segmentation.nii.gz
│
├───RUNMC
│       Case00.nii.gz
│       Case00_segmentation.nii.gz
│       Case01.nii.gz
│       Case01_segmentation.nii.gz
│       Case02.nii.gz
│       Case02_segmentation.nii.gz
│       Case03.nii.gz
│       Case03_segmentation.nii.gz
│       Case04.nii.gz
│       Case04_segmentation.nii.gz
│       Case05.nii.gz
│       Case05_segmentation.nii.gz
│       Case06.nii.gz
│       Case06_segmentation.nii.gz
│       Case07.nii.gz
│       Case07_segmentation.nii.gz
│       Case08.nii.gz
│       Case08_segmentation.nii.gz
│       Case09.nii.gz
│       Case09_segmentation.nii.gz
│       Case10.nii.gz
│       Case10_segmentation.nii.gz
│       Case11.nii.gz
│       Case11_segmentation.nii.gz
│       Case12.nii.gz
│       Case12_segmentation.nii.gz
│       Case13.nii.gz
│       Case13_segmentation.nii.gz
│       Case14.nii.gz
│       Case14_segmentation.nii.gz
│       Case15.nii.gz
│       Case15_segmentation.nii.gz
│       Case16.nii.gz
│       Case16_segmentation.nii.gz
│       Case17.nii.gz
│       Case17_segmentation.nii.gz
│       Case18.nii.gz
│       Case18_segmentation.nii.gz
│       Case19.nii.gz
│       Case19_segmentation.nii.gz
│       Case20.nii.gz
│       Case20_segmentation.nii.gz
│       Case21.nii.gz
│       Case21_segmentation.nii.gz
│       Case22.nii.gz
│       Case22_segmentation.nii.gz
│       Case23.nii.gz
│       Case23_segmentation.nii.gz
│       Case24.nii.gz
│       Case24_segmentation.nii.gz
│       Case25.nii.gz
│       Case25_segmentation.nii.gz
│       Case26.nii.gz
│       Case26_segmentation.nii.gz
│       Case27.nii.gz
│       Case27_segmentation.nii.gz
│       Case28.nii.gz
│       Case28_segmentation.nii.gz
│       Case29.nii.gz
│       Case29_segmentation.nii.gz
│
└───UCL
        Case01.nii.gz
        Case01_segmentation.nii.gz
        Case26.nii.gz
        Case26_segmentation.nii.gz
        Case27.nii.gz
        Case27_segmentation.nii.gz
        Case28.nii.gz
        Case28_segmentation.nii.gz
        Case29.nii.gz
        Case29_segmentation.nii.gz
        Case30.nii.gz
        Case30_segmentation.nii.gz
        Case31.nii.gz
        Case31_segmentation.nii.gz
        Case32.nii.gz
        Case32_segmentation.nii.gz
        Case33.nii.gz
        Case33_segmentation.nii.gz
        Case34.nii.gz
        Case34_segmentation.nii.gz
        Case35.nii.gz
        Case35_segmentation.nii.gz
        Case36.nii.gz
        Case36_segmentation.nii.gz
        Case37.nii.gz
        Case37_segmentation.nii.gz


(masam) D:\Anum\MA-SAM>python preprocessing\preprocess.py
Data organization complete!
3D → 2D slice conversion complete!
CSV files generated!
Preprocessing complete!

(masam) D:\Anum\MA-SAM>python MA-SAM/train.py --root_path data_train/prostateD/2D_all_5slice/ --output output/ --ckpt sam_vit_h_4b8939.pth --batch_size 1 --n_gpu 1 --img_size 256 --use_amp
D:\Anum\MA-SAM\MA-SAM\segment_anything\build_sam.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(f)
Namespace(root_path='data_train/prostateD/2D_all_5slice/', output='output/', num_classes=12, batch_size=1, n_gpu=1, base_lr=0.0008, max_epochs=400, stop_epoch=300, deterministic=1, img_size=256, seed=1234, vit_name='vit_h', ckpt='sam_vit_h_4b8939.pth', adapt_ckpt=None, rank=32, scale=1.0, warmup=True, warmup_period=250, AdamW=True, module='sam_fact_tt_image_encoder', dice_param=0.8, lr_exp=7, tf32=True, compile=False, use_amp=True, skip_hard=True)
The length of train set is: 3744
D:\Anum\MA-SAM\MA-SAM\trainer.py:73: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.use_amp)
3744 iterations per epoch. 1497600 max iterations
  0%|                                         | 0/400 [00:00<?, ?it/s]iteration 1 : loss : 1.261724, loss_ce: 2.440067, loss_dice: 0.967138
iteration 2 : loss : 1.240524, loss_ce: 2.328410, loss_dice: 0.968552
iteration 3 : loss : 1.221730, loss_ce: 2.244462, loss_dice: 0.966047
iteration 4 : loss : 1.268841, loss_ce: 2.458172, loss_dice: 0.971508
iteration 5 : loss : 1.183860, loss_ce: 2.100651, loss_dice: 0.954662
iteration 6 : loss : 1.144682, loss_ce: 1.935010, loss_dice: 0.947100
iteration 7 : loss : 1.163615, loss_ce: 2.008448, loss_dice: 0.952407
iteration 8 : loss : 1.099119, loss_ce: 1.743243, loss_dice: 0.938089
iteration 9 : loss : 1.076179, loss_ce: 1.648643, loss_dice: 0.933062
iteration 10 : loss : 1.028440, loss_ce: 1.401816, loss_dice: 0.935096
iteration 11 : loss : 0.996034, loss_ce: 1.251951, loss_dice: 0.932054
iteration 12 : loss : 0.966377, loss_ce: 1.100031, loss_dice: 0.932964
iteration 13 : loss : 0.951060, loss_ce: 1.036811, loss_dice: 0.929622
iteration 14 : loss : 0.963754, loss_ce: 1.108637, loss_dice: 0.927533
iteration 15 : loss : 1.000172, loss_ce: 1.289070, loss_dice: 0.927948
iteration 16 : loss : 0.914482, loss_ce: 0.868095, loss_dice: 0.926079
iteration 17 : loss : 0.832630, loss_ce: 0.451803, loss_dice: 0.927837
iteration 18 : loss : 0.813297, loss_ce: 0.358020, loss_dice: 0.927116
iteration 19 : loss : 0.953471, loss_ce: 1.055614, loss_dice: 0.927935
iteration 20 : loss : 0.794216, loss_ce: 0.271519, loss_dice: 0.924890
iteration 21 : loss : 0.790733, loss_ce: 0.254503, loss_dice: 0.924791
iteration 22 : loss : 0.775538, loss_ce: 0.177143, loss_dice: 0.925137
iteration 23 : loss : 0.908154, loss_ce: 0.833631, loss_dice: 0.926784
iteration 24 : loss : 0.835240, loss_ce: 0.475204, loss_dice: 0.925249
iteration 25 : loss : 0.783459, loss_ce: 0.223012, loss_dice: 0.923571
iteration 26 : loss : 0.751702, loss_ce: 0.064178, loss_dice: 0.923583
iteration 27 : loss : 0.749897, loss_ce: 0.055680, loss_dice: 0.923451
iteration 28 : loss : 0.756615, loss_ce: 0.092169, loss_dice: 0.922726
iteration 29 : loss : 0.811418, loss_ce: 0.360937, loss_dice: 0.924038
iteration 30 : loss : 0.840836, loss_ce: 0.505803, loss_dice: 0.924594
iteration 31 : loss : 0.788313, loss_ce: 0.247182, loss_dice: 0.923595
iteration 32 : loss : 0.774470, loss_ce: 0.180044, loss_dice: 0.923077
iteration 33 : loss : 0.760894, loss_ce: 0.112943, loss_dice: 0.922881
iteration 34 : loss : 0.765264, loss_ce: 0.135472, loss_dice: 0.922712
iteration 35 : loss : 0.788495, loss_ce: 0.249282, loss_dice: 0.923298
iteration 36 : loss : 0.776688, loss_ce: 0.192105, loss_dice: 0.922834
iteration 37 : loss : 0.804889, loss_ce: 0.330635, loss_dice: 0.923453
iteration 38 : loss : 0.797852, loss_ce: 0.297796, loss_dice: 0.922866
iteration 39 : loss : 0.758629, loss_ce: 0.104009, loss_dice: 0.922284
iteration 40 : loss : 0.791249, loss_ce: 0.265162, loss_dice: 0.922770
iteration 41 : loss : 0.859769, loss_ce: 0.598337, loss_dice: 0.925127
iteration 42 : loss : 0.768696, loss_ce: 0.154232, loss_dice: 0.922312
iteration 43 : loss : 0.754251, loss_ce: 0.083466, loss_dice: 0.921947
iteration 44 : loss : 0.752219, loss_ce: 0.067505, loss_dice: 0.923397
iteration 45 : loss : 0.794460, loss_ce: 0.278292, loss_dice: 0.923502
iteration 46 : loss : 0.752272, loss_ce: 0.071306, loss_dice: 0.922514
iteration 47 : loss : 0.741453, loss_ce: 0.014895, loss_dice: 0.923092
iteration 48 : loss : 0.795611, loss_ce: 0.282104, loss_dice: 0.923988
iteration 49 : loss : 0.784762, loss_ce: 0.228797, loss_dice: 0.923753
iteration 50 : loss : 0.740628, loss_ce: 0.010817, loss_dice: 0.923081
iteration 51 : loss : 0.740812, loss_ce: 0.011727, loss_dice: 0.923083
iteration 52 : loss : 0.740147, loss_ce: 0.008468, loss_dice: 0.923067
iteration 53 : loss : 0.739608, loss_ce: 0.005855, loss_dice: 0.923046
iteration 54 : loss : 0.739849, loss_ce: 0.007049, loss_dice: 0.923049
iteration 55 : loss : 0.757941, loss_ce: 0.097060, loss_dice: 0.923162
iteration 56 : loss : 0.738945, loss_ce: 0.003127, loss_dice: 0.922899
iteration 57 : loss : 0.738955, loss_ce: 0.003274, loss_dice: 0.922875
iteration 58 : loss : 0.775537, loss_ce: 0.183911, loss_dice: 0.923444
iteration 59 : loss : 0.779144, loss_ce: 0.202336, loss_dice: 0.923346
iteration 60 : loss : 0.866970, loss_ce: 0.632246, loss_dice: 0.925651
iteration 61 : loss : 0.782150, loss_ce: 0.218861, loss_dice: 0.922972
iteration 62 : loss : 0.780701, loss_ce: 0.221934, loss_dice: 0.920392
iteration 63 : loss : 0.759958, loss_ce: 0.105055, loss_dice: 0.923684
iteration 64 : loss : 0.766342, loss_ce: 0.147706, loss_dice: 0.921000
iteration 65 : loss : 0.759161, loss_ce: 0.101246, loss_dice: 0.923640
iteration 66 : loss : 0.778851, loss_ce: 0.213058, loss_dice: 0.920299
iteration 67 : loss : 0.774123, loss_ce: 0.184917, loss_dice: 0.921424
iteration 68 : loss : 0.767501, loss_ce: 0.149870, loss_dice: 0.921909
iteration 69 : loss : 0.743963, loss_ce: 0.034789, loss_dice: 0.921256
iteration 70 : loss : 0.740991, loss_ce: 0.017563, loss_dice: 0.921848
iteration 71 : loss : 0.747101, loss_ce: 0.047229, loss_dice: 0.922069
iteration 72 : loss : 0.739105, loss_ce: 0.006321, loss_dice: 0.922300
iteration 73 : loss : 0.737913, loss_ce: 0.004783, loss_dice: 0.921196
iteration 74 : loss : 0.732668, loss_ce: 0.002775, loss_dice: 0.915141
iteration 75 : loss : 0.761404, loss_ce: 0.272943, loss_dice: 0.883519
iteration 76 : loss : 0.406658, loss_ce: 0.152967, loss_dice: 0.470080
iteration 77 : loss : 0.372947, loss_ce: 0.036463, loss_dice: 0.457069
iteration 78 : loss : 0.358148, loss_ce: 0.076689, loss_dice: 0.428513
iteration 79 : loss : 0.373813, loss_ce: 0.000410, loss_dice: 0.467163
iteration 80 : loss : 0.009848, loss_ce: 0.000001, loss_dice: 0.012310
iteration 81 : loss : 0.000000, loss_ce: 0.000000, loss_dice: 0.000000
iteration 82 : loss : 0.203799, loss_ce: 0.708176, loss_dice: 0.077705
iteration 83 : loss : 0.079445, loss_ce: 0.089186, loss_dice: 0.077009
iteration 84 : loss : 0.147612, loss_ce: 0.428871, loss_dice: 0.077298
iteration 85 : loss : 0.061631, loss_ce: 0.000449, loss_dice: 0.076927
iteration 86 : loss : 0.062714, loss_ce: 0.005595, loss_dice: 0.076994
iteration 87 : loss : 0.062000, loss_ce: 0.002208, loss_dice: 0.076948
iteration 88 : loss : 0.384461, loss_ce: 1.609846, loss_dice: 0.078114
iteration 89 : loss : 0.433836, loss_ce: 1.856121, loss_dice: 0.078264
iteration 90 : loss : 0.134874, loss_ce: 0.367655, loss_dice: 0.076679
iteration 91 : loss : 0.309016, loss_ce: 1.236438, loss_dice: 0.077161
iteration 92 : loss : 0.112499, loss_ce: 0.014133, loss_dice: 0.137090
iteration 93 : loss : 0.174799, loss_ce: 0.567361, loss_dice: 0.076659
iteration 94 : loss : 0.065791, loss_ce: 0.020245, loss_dice: 0.077177
iteration 95 : loss : 0.237544, loss_ce: 0.876832, loss_dice: 0.077722
iteration 96 : loss : 0.061548, loss_ce: 0.000069, loss_dice: 0.076917
iteration 97 : loss : 0.087319, loss_ce: 0.128360, loss_dice: 0.077059
iteration 98 : loss : 0.131878, loss_ce: 0.350068, loss_dice: 0.077331
iteration 99 : loss : 0.061557, loss_ce: 0.000106, loss_dice: 0.076920
iteration 100 : loss : 0.085002, loss_ce: 0.116831, loss_dice: 0.077044
iteration 101 : loss : 0.162911, loss_ce: 0.504365, loss_dice: 0.077548
iteration 102 : loss : 0.062081, loss_ce: 0.002631, loss_dice: 0.076943
iteration 103 : loss : 0.062596, loss_ce: 0.005083, loss_dice: 0.076975
iteration 104 : loss : 0.062667, loss_ce: 0.005437, loss_dice: 0.076974
iteration 105 : loss : 0.368740, loss_ce: 1.526163, loss_dice: 0.079384
iteration 106 : loss : 0.072623, loss_ce: 0.057866, loss_dice: 0.076312
iteration 107 : loss : 0.067998, loss_ce: 0.031025, loss_dice: 0.077242
iteration 108 : loss : 0.138889, loss_ce: 0.393016, loss_dice: 0.075357
iteration 109 : loss : 0.066903, loss_ce: 0.025961, loss_dice: 0.077139
iteration 110 : loss : 0.108883, loss_ce: 0.240523, loss_dice: 0.075973
iteration 111 : loss : 0.064123, loss_ce: 0.012525, loss_dice: 0.077022
iteration 112 : loss : 0.063324, loss_ce: 0.008579, loss_dice: 0.077010
iteration 113 : loss : 0.062577, loss_ce: 0.004983, loss_dice: 0.076975
iteration 114 : loss : 0.072612, loss_ce: 0.056029, loss_dice: 0.076758
iteration 115 : loss : 0.063766, loss_ce: 0.012165, loss_dice: 0.076666
iteration 116 : loss : 0.062225, loss_ce: 0.002647, loss_dice: 0.077119
iteration 117 : loss : 0.066468, loss_ce: 0.023554, loss_dice: 0.077196
iteration 118 : loss : 0.121539, loss_ce: 0.295833, loss_dice: 0.077966
iteration 119 : loss : 0.062837, loss_ce: 0.004906, loss_dice: 0.077320
iteration 120 : loss : 0.062992, loss_ce: 0.006336, loss_dice: 0.077156
iteration 121 : loss : 0.259483, loss_ce: 0.974238, loss_dice: 0.080794
iteration 122 : loss : 0.121252, loss_ce: 0.301436, loss_dice: 0.076207
iteration 123 : loss : 0.105489, loss_ce: 0.228757, loss_dice: 0.074671
iteration 124 : loss : 0.128048, loss_ce: 0.310160, loss_dice: 0.082520
iteration 125 : loss : 0.115845, loss_ce: 0.294130, loss_dice: 0.071274
iteration 126 : loss : 0.074151, loss_ce: 0.060397, loss_dice: 0.077590
iteration 127 : loss : 0.065601, loss_ce: 0.026546, loss_dice: 0.075364
iteration 128 : loss : 0.081924, loss_ce: 0.101981, loss_dice: 0.076910
iteration 129 : loss : 0.061986, loss_ce: 0.001978, loss_dice: 0.076989
iteration 130 : loss : 0.117406, loss_ce: 0.274458, loss_dice: 0.078143
iteration 131 : loss : 0.061753, loss_ce: 0.000843, loss_dice: 0.076980
iteration 132 : loss : 0.139168, loss_ce: 0.381423, loss_dice: 0.078604
iteration 133 : loss : 0.065152, loss_ce: 0.017983, loss_dice: 0.076944
iteration 134 : loss : 0.068145, loss_ce: 0.032704, loss_dice: 0.077005
iteration 135 : loss : 0.107553, loss_ce: 0.226057, loss_dice: 0.077927
iteration 136 : loss : 0.062261, loss_ce: 0.002526, loss_dice: 0.077195
iteration 137 : loss : 0.159286, loss_ce: 0.479535, loss_dice: 0.079224
iteration 138 : loss : 0.080039, loss_ce: 0.093294, loss_dice: 0.076725
iteration 139 : loss : 0.114707, loss_ce: 0.266919, loss_dice: 0.076654
iteration 140 : loss : 0.074217, loss_ce: 0.056441, loss_dice: 0.078661
iteration 141 : loss : 0.122100, loss_ce: 0.317014, loss_dice: 0.073371
iteration 142 : loss : 0.099205, loss_ce: 0.206505, loss_dice: 0.072380
iteration 143 : loss : 0.075310, loss_ce: 0.062690, loss_dice: 0.078465
iteration 144 : loss : 0.067298, loss_ce: 0.034929, loss_dice: 0.075390
iteration 145 : loss : 0.063841, loss_ce: 0.009941, loss_dice: 0.077316
iteration 146 : loss : 0.091182, loss_ce: 0.146468, loss_dice: 0.077360
iteration 147 : loss : 0.062218, loss_ce: 0.002863, loss_dice: 0.077057
iteration 148 : loss : 0.062013, loss_ce: 0.002057, loss_dice: 0.077002
iteration 149 : loss : 0.067056, loss_ce: 0.027857, loss_dice: 0.076856
iteration 150 : loss : 0.065805, loss_ce: 0.021625, loss_dice: 0.076850
iteration 151 : loss : 0.088870, loss_ce: 0.134845, loss_dice: 0.077376
iteration 152 : loss : 0.146524, loss_ce: 0.418050, loss_dice: 0.078643
iteration 153 : loss : 0.135974, loss_ce: 0.366325, loss_dice: 0.078386
iteration 154 : loss : 0.148688, loss_ce: 0.429009, loss_dice: 0.078608
iteration 155 : loss : 0.119516, loss_ce: 0.289406, loss_dice: 0.077043
iteration 156 : loss : 0.169320, loss_ce: 0.543774, loss_dice: 0.075707
iteration 157 : loss : 0.103392, loss_ce: 0.197850, loss_dice: 0.079778
iteration 158 : loss : 0.097453, loss_ce: 0.199064, loss_dice: 0.072050
iteration 159 : loss : 0.088786, loss_ce: 0.139345, loss_dice: 0.076147
iteration 160 : loss : 0.067012, loss_ce: 0.025868, loss_dice: 0.077298
iteration 161 : loss : 0.071773, loss_ce: 0.053319, loss_dice: 0.076386
iteration 162 : loss : 0.082578, loss_ce: 0.103413, loss_dice: 0.077370
iteration 163 : loss : 0.128154, loss_ce: 0.326014, loss_dice: 0.078689
iteration 164 : loss : 0.082794, loss_ce: 0.104455, loss_dice: 0.077379
iteration 165 : loss : 0.077068, loss_ce: 0.077171, loss_dice: 0.077043
iteration 166 : loss : 0.062296, loss_ce: 0.003443, loss_dice: 0.077009
iteration 167 : loss : 0.062468, loss_ce: 0.004353, loss_dice: 0.076997
iteration 168 : loss : 0.098755, loss_ce: 0.185431, loss_dice: 0.077087
iteration 169 : loss : 0.102623, loss_ce: 0.205390, loss_dice: 0.076932
iteration 170 : loss : 0.063666, loss_ce: 0.010233, loss_dice: 0.077024
iteration 171 : loss : 0.064099, loss_ce: 0.012377, loss_dice: 0.077029
iteration 172 : loss : 0.063993, loss_ce: 0.011927, loss_dice: 0.077010
iteration 173 : loss : 0.086652, loss_ce: 0.128308, loss_dice: 0.076238
iteration 174 : loss : 0.063150, loss_ce: 0.007909, loss_dice: 0.076961
iteration 175 : loss : 0.091591, loss_ce: 0.150845, loss_dice: 0.076778
iteration 176 : loss : 0.100002, loss_ce: 0.191802, loss_dice: 0.077052
iteration 177 : loss : 0.062721, loss_ce: 0.005838, loss_dice: 0.076942
iteration 178 : loss : 0.144635, loss_ce: 0.410374, loss_dice: 0.078200
iteration 179 : loss : 0.063358, loss_ce: 0.008952, loss_dice: 0.076960
iteration 180 : loss : 0.066620, loss_ce: 0.030683, loss_dice: 0.075605
iteration 181 : loss : 0.064427, loss_ce: 0.014017, loss_dice: 0.077029
iteration 182 : loss : 0.094905, loss_ce: 0.170307, loss_dice: 0.076054
iteration 183 : loss : 0.105467, loss_ce: 0.222483, loss_dice: 0.076213
iteration 184 : loss : 0.066895, loss_ce: 0.024587, loss_dice: 0.077472
iteration 185 : loss : 0.067470, loss_ce: 0.026483, loss_dice: 0.077717
iteration 186 : loss : 0.066510, loss_ce: 0.021232, loss_dice: 0.077830
iteration 187 : loss : 0.080086, loss_ce: 0.095501, loss_dice: 0.076232
iteration 188 : loss : 0.064067, loss_ce: 0.010472, loss_dice: 0.077466
iteration 189 : loss : 0.063279, loss_ce: 0.007379, loss_dice: 0.077254
iteration 190 : loss : 0.099196, loss_ce: 0.185666, loss_dice: 0.077579
iteration 191 : loss : 0.092254, loss_ce: 0.151882, loss_dice: 0.077347
iteration 192 : loss : 0.065950, loss_ce: 0.024454, loss_dice: 0.076324
iteration 193 : loss : 0.088021, loss_ce: 0.132277, loss_dice: 0.076956
iteration 194 : loss : 0.063433, loss_ce: 0.009145, loss_dice: 0.077005
iteration 195 : loss : 0.125291, loss_ce: 0.315214, loss_dice: 0.077810
iteration 196 : loss : 0.066862, loss_ce: 0.026218, loss_dice: 0.077023
iteration 197 : loss : 0.070115, loss_ce: 0.042285, loss_dice: 0.077073
iteration 198 : loss : 0.070235, loss_ce: 0.042881, loss_dice: 0.077074
iteration 199 : loss : 0.067297, loss_ce: 0.028393, loss_dice: 0.077023
iteration 200 : loss : 0.064633, loss_ce: 0.015197, loss_dice: 0.076992
iteration 201 : loss : 0.063147, loss_ce: 0.007824, loss_dice: 0.076978
iteration 202 : loss : 0.065839, loss_ce: 0.023319, loss_dice: 0.076469
iteration 203 : loss : 0.092735, loss_ce: 0.153501, loss_dice: 0.077543
iteration 204 : loss : 0.061918, loss_ce: 0.001754, loss_dice: 0.076959
iteration 205 : loss : 0.076883, loss_ce: 0.075727, loss_dice: 0.077173
iteration 206 : loss : 0.072163, loss_ce: 0.052563, loss_dice: 0.077064
iteration 207 : loss : 0.070460, loss_ce: 0.044180, loss_dice: 0.077030
iteration 208 : loss : 0.061773, loss_ce: 0.001030, loss_dice: 0.076959
iteration 209 : loss : 0.092270, loss_ce: 0.150862, loss_dice: 0.077622
iteration 210 : loss : 0.061802, loss_ce: 0.001139, loss_dice: 0.076968
iteration 211 : loss : 0.061834, loss_ce: 0.001270, loss_dice: 0.076975
iteration 212 : loss : 0.061860, loss_ce: 0.001384, loss_dice: 0.076979
iteration 213 : loss : 0.061880, loss_ce: 0.001478, loss_dice: 0.076980
iteration 214 : loss : 0.061891, loss_ce: 0.001544, loss_dice: 0.076978
iteration 215 : loss : 0.061896, loss_ce: 0.001586, loss_dice: 0.076973
iteration 216 : loss : 0.114580, loss_ce: 0.259914, loss_dice: 0.078246
iteration 217 : loss : 0.062025, loss_ce: 0.002195, loss_dice: 0.076983
iteration 218 : loss : 0.098215, loss_ce: 0.180258, loss_dice: 0.077705
iteration 219 : loss : 0.068829, loss_ce: 0.037857, loss_dice: 0.076572
iteration 220 : loss : 0.063119, loss_ce: 0.007214, loss_dice: 0.077095
iteration 221 : loss : 0.088062, loss_ce: 0.133980, loss_dice: 0.076583
iteration 222 : loss : 0.073307, loss_ce: 0.066004, loss_dice: 0.075133
iteration 223 : loss : 0.089474, loss_ce: 0.149565, loss_dice: 0.074451
iteration 224 : loss : 0.070743, loss_ce: 0.055522, loss_dice: 0.074548
iteration 225 : loss : 0.073030, loss_ce: 0.056151, loss_dice: 0.077249
iteration 226 : loss : 0.091757, loss_ce: 0.166828, loss_dice: 0.072990
iteration 227 : loss : 0.068121, loss_ce: 0.032548, loss_dice: 0.077015
iteration 228 : loss : 0.065811, loss_ce: 0.021204, loss_dice: 0.076963


...


iteration 4030 : loss : 0.102025, loss_ce: 0.205781, loss_dice: 0.076086
iteration 4031 : loss : 0.072825, loss_ce: 0.066073, loss_dice: 0.074513
iteration 4032 : loss : 0.065848, loss_ce: 0.021477, loss_dice: 0.076941
iteration 4033 : loss : 0.100772, loss_ce: 0.202432, loss_dice: 0.075356
iteration 4034 : loss : 0.065541, loss_ce: 0.030289, loss_dice: 0.074354
iteration 4035 : loss : 0.096643, loss_ce: 0.184768, loss_dice: 0.074612
iteration 4036 : loss : 0.088152, loss_ce: 0.145458, loss_dice: 0.073826
iteration 4037 : loss : 0.079556, loss_ce: 0.105808, loss_dice: 0.072993
iteration 4038 : loss : 0.068342, loss_ce: 0.049992, loss_dice: 0.072930

(masam) D:\Anum\MA-SAM\MA-SAM>python test.py --adapt_ckpt ../prostate_siteE_modelweights.pth --data_path ../data_train/prostateD/2D_all_5slice/ --ckpt ../sam_vit_h_4b8939.pth
D:\Anum\MA-SAM\MA-SAM\segment_anything\build_sam.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(f)
D:\Anum\MA-SAM\MA-SAM\sam_fact_tt_image_encoder.py:389: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(filename)
Skipping mask_decoder key mask_decoder.mask_tokens.weight: shape mismatch torch.Size([2, 256]) vs torch.Size([13, 256])
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.2.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.2.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.2.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.2.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.2.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.2.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.3.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.3.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.3.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.3.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.3.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.3.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.4.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.4.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.4.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.4.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.4.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.4.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.5.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.5.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.5.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.5.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.5.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.5.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.6.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.6.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.6.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.6.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.6.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.6.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.7.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.7.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.7.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.7.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.7.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.7.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.8.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.8.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.8.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.8.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.8.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.8.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.9.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.9.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.9.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.9.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.9.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.9.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.10.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.10.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.10.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.10.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.10.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.10.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.11.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.11.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.11.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.11.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.11.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.11.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.12.layers.0.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.12.layers.0.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.12.layers.1.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.12.layers.1.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.12.layers.2.weight: not found in checkpoint
Skipping mask_decoder key mask_decoder.output_hypernetworks_mlps.12.layers.2.bias: not found in checkpoint
Skipping mask_decoder key mask_decoder.iou_prediction_head.layers.2.weight: shape mismatch torch.Size([2, 256]) vs torch.Size([13, 256])
Skipping mask_decoder key mask_decoder.iou_prediction_head.layers.2.bias: shape mismatch torch.Size([2]) vs torch.Size([13])
Namespace(adapt_ckpt='../prostate_siteE_modelweights.pth', data_path='../data_train/prostateD/2D_all_5slice/', num_classes=12, img_size=512, seed=1234, is_savenii=False, deterministic=1, ckpt='../sam_vit_h_4b8939.pth', vit_name='vit_h', rank=32, scale=1.0, module='sam_fact_tt_image_encoder', output_dir='../prostate_siteE_modelweights')
  0%|                                                                                            | 0/3 [00:00<?, ?it/s]idx 1 case BIDMC_Case00 mean_dice 0.076894
 33%|████████████████████████████                                                        | 1/3 [00:32<01:04, 32.19s/it]idx 1 case BIDMC_Case02 mean_dice 0.078746
 67%|████████████████████████████████████████████████████████                            | 2/3 [01:08<00:34, 34.90s/it]idx 1 case BIDMC_Case03 mean_dice 0.078733
100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:37<00:00, 32.52s/it]
Mean class 1 name spleen mean_dice 0.937492
Mean class 2 name right kidney mean_dice 0.000000
Mean class 3 name left kidney mean_dice 0.000000
Mean class 4 name gallbladder mean_dice 0.000000
Mean class 5 name esophagus mean_dice 0.000000
Mean class 6 name liver mean_dice 0.000000
Mean class 7 name stomach mean_dice 0.000000
Mean class 8 name aorta mean_dice 0.000000
Mean class 9 name vena mean_dice 0.000000
Mean class 10 name vein mean_dice 0.000000
Mean class 11 name pancreas mean_dice 0.000000
Mean class 12 name adrenal gland mean_dice 0.000000
Testing performance in best val model: mean_dice : 0.078124
Testing Finished!
